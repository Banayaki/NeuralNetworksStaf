{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Импорт классики\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ softmax(x)_j=\\frac{exp(x_j)}{\\Sigma_i exp(x_i)} $$\n",
    "<br>\n",
    "$$ H_t(y) = - \\Sigma_i t_i \\log y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заглушка, которуб мы заполним, когда TensorFlow будет вычислять\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_pred = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Переменные весов и смещений\n",
    "w = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Модель собственной персоной\n",
    "y = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "# Функция ошибки - cross-entropy \n",
    "#(сначала вычисляем логарифм каждого элемента, затем y_pred \n",
    "# умножаем на соотвествующий ему логарифм, и затем суммираем результат \n",
    "# по второму измерению)\n",
    "cross_entropy = tf.reduce_mean(- tf.reduce_sum(y_pred  * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# Градиентный спуск с learn_rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Запуск сессии в интерактивном режиме и инициализация пременных\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_pred: batch_ys})\n",
    "    pass\n",
    "\n",
    "# Получаем на выходе список булевых значений\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1))\n",
    "# Тест\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: %s\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_pred: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усовершенствованная модель, со скрытым слоем и dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9750001\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_pred = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Инициализируем переменные небольшими рандомными значениями\n",
    "w_relu = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1))\n",
    "b_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1))\n",
    "\n",
    "# Скрытый слой в общем виде (ReLU)\n",
    "h = tf.nn.relu(tf.matmul(x, w_relu) + b_relu)\n",
    "\n",
    "# Dropout\n",
    "keep_probability = tf.placeholder(tf.float32)\n",
    "h_drop = tf.nn.dropout(h, keep_probability)\n",
    "\n",
    "# Третий слой (выход)\n",
    "w = tf.Variable(tf.zeros([100, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# Softmax с небольшим улучшением, что бы не получить ноль в знаменателе\n",
    "# Избавимся от деления на ноль \n",
    "y = tf.nn.softmax(tf.matmul(h_drop, w) + b - tf.reduce_max(x))\n",
    "\n",
    "\n",
    "logit = tf.matmul(h_drop, w) + b \n",
    "# Разработчики молодцы, лучше применить эту функцию что бы избежать деления на 0\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y_pred))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_pred: batch_ys, keep_probability: 0.5})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: %s\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_pred: mnist.test.labels, keep_probability: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
